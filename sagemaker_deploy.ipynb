{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Facts\n",
    "## Leveraging google search summaries and the googlesearch API to evaluate factual statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (i) Project Overview\n",
    "One of the things that makes machine learning so powerful is its ability to classify data sets based on very weak signalling of information contained therein. This is the somewhat 'magical' ability that ML has to identify associations where there seems to be only noisy data. One of the problems with using machine learning models to identify true statements is that both true and false statements can be structured in an identical manner, and the quality of being 'true' or 'false' is only verifiable with outside information. For example, if I say 'Sean Connery was born in England' you have no way of evaluating the truthfulness of this statement without having an extrinsic reference. In other words, from the point of view of language in and of itself, this statement is identical to the the truth. No matter how goodd our machine learning models are, this statment has no information, weakly signalled or othwerise, to indicate whether or not its true and thus the classifier has nothing to work with. \n",
    "\n",
    "Where was Sean Connery born? I correctly assumed Scotland, but I had to double-check on wikipedia, i.e. an extrinsic source. One can also simply google 'Where was Sean Connery born?' and they'll receive an accurate answer based on Google's relevance algorithms. Finding extrinsic information for claims like this is trivial for a human with today's web, but cumbersome if we want to verify statements automatically. Furthermore, google results sometimes give conflicting information or obscure results that can be time consuming for a human to evaluate. But something is almost always true - Google gives *some* kind of result that is related to the query by relevance. It occurred to me that poring through vaguely related text to find weakly signalled extrinsic information is precisely the sort of task neural nets are good at performing. Thus this project is simply an attempt to classify statements as true or false by using the statement itself, and a summary of google search results based on the statement, as the feature space to train a moddel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii) Model structure and Data source\n",
    "The model used for this classifier is the famous BERT (Bi-directional Encoder Representations from Transformers) model, which was produced by Google and released as an open source tool to the public. BERT is a natural language model that uses some recent developments in machine learning research to create a model that is extremely good at finding and using the relationships in long strings of text, even if they are far apart. This is precisely the kind of power I needed for this model, since a small snippet of text in the google search summary might be the signal that relates to the query statement and verifies or discards it. For example, if we search \"Sean Connery was Born in England,\" one of the search results might contain 'born in Scotland'. BERT can then learn to pay attention to the relationship between those two phrases, and then in the process of training the model, it would hopefully learn that dissimilarity between the two phrases 'born in ___' is related to the false classification value. \n",
    "\n",
    "The dataset for this project comes from FEVER (Fact Extraction and VERification) dataset, a large set of statements that were tagged as verfied/non-verified/unverifiable via human examination (*Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit*, 2018). Code for this project was adapted from [this](https://towardsdatascience.com/steps-to-start-training-your-custom-tensorflow-model-in-aws-sagemaker-ae9cf7a205b9) article and [this](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) article."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (iii) Test model locally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import deploy_utils\n",
    "import time\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import keras\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from transformers import TFBertForSequenceClassification\n",
    "import argparse\n",
    "import json\n",
    "from importlib import reload\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# import boto3\n",
    "import utils\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('data/models/transformer_model/')\n",
    "\n",
    "# Test query\n",
    "query = \"The fastest growing population in the United States is Hispanic\"\n",
    "\n",
    "clean_query, err_code = deploy_utils.get_input(query)\n",
    "\n",
    "if err_code:\n",
    "    output = 'There was an error searching your claim, most likely due to monthly quotas with the Google Search API'\n",
    "else:\n",
    "    input_id, attn_mask = deploy_utils.bert_tokenize(bert_tokenizer, clean_query)\n",
    "\n",
    "pred = bert_model([input_id, attn_mask])\n",
    "logit_softmax = tf.nn.softmax(pred.logits).numpy()\n",
    "print(logit_softmax)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (iv) Code to deploy model for training on sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "## Create sagemaker session and role"
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a SageMaker session to work with\n",
    "sagemaker_session = sagemaker.Session()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='trent_sage')['Role']['Arn']\n",
    "    region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "print(role)\n",
    "print(region)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_folder_name='data'\n",
    "train_filename = 'cleaned_data.pickle'\n",
    "# Set the directories for our model output\n",
    "trainedmodel_path = 'trained_model'\n",
    "output_data_path = 'output_data'\n",
    "# Set the name of the artifacts that our model generate (model not included) \n",
    "model_info_file = 'model_info.pth'\n",
    "train_file = os.path.abspath(os.path.join(data_folder_name, train_filename))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Specify bucket name\n",
    "bucket_name = 'zennsunni-ml-sagemaker'\n",
    "# Set the training data folder in S3\n",
    "training_folder = r'true_facts/train'\n",
    "# Set the output folder in S3\n",
    "output_folder = r'true_facts'\n",
    "# Set the checkpoint in S3 folder for our model \n",
    "ckpt_folder = r'true_facts/ckpt'\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_folder\n",
    "output_data_uri = r's3://' + bucket_name + r'/' + output_folder\n",
    "ckpt_data_uri = r's3://' + bucket_name + r'/' + ckpt_folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload training data and file structure to S3 "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "# (v) Creating and fitting the Sagemaker estimator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The instance type to use. This is the cheapest single GPU instance.\n",
    "# instance_type='ml.m5.2xlarge'\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "# instance_type='local'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Tensorflow estimator using a Tensorflow 2.1 container\n",
    "\n",
    "\n",
    "### examine parameters\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir=\"train\",\n",
    "                       # requirements_file='requirements.txt',\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       framework_version='2.2.0',\n",
    "                       py_version='py37',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       #checkpoint_local_path = 'ckpt', #Use default value /opt/ml/checkpoint\n",
    "                       checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'resume': True,\n",
    "                        'train_file': 'cleaned_data.pickle',\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "truefacts-2021-03-06-18-26-19\n"
     ]
    }
   ],
   "source": [
    "# Set the job name and show it\n",
    "job_name = 'truefacts-{}'.format(time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Call the fit method to launch the training job\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
